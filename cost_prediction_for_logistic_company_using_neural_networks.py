# -*- coding: utf-8 -*-
"""Cost Prediction For Logistic Company Using Neural Networks

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UQWXEafjqZs6FsgW7nlTzqLre6WCOnKS

#**Cost Prediction For Logistic Company**

**Problem Statement:** To optimize the delivery network of a logistics company, the dataset is provided with delivery information and associated trip costs.

**Task:** To build a model to predict the cost of deliveries.

# **Importing Libraries**
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.model_selection import cross_val_predict, KFold
from sklearn.decomposition import PCA
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.linear_model import Ridge
from sklearn.linear_model import BayesianRidge
import random
from pandas.plotting import scatter_matrix
from sklearn.svm import SVR
from sklearn.linear_model import Ridge
import os
import tarfile
from six.moves import urllib

# Importing tensorflow and keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense, Softmax, Dropout
from tensorflow.keras import optimizers

# Checking the tensorflow libraries
print(tf.__version__)

"""#**Data Extraction**"""

#Read train.csv file into pandas dataframe
dataset_train = pd.read_csv("/content/sample_data/train.csv")

#dataset_train = pd.read_csv(file_url_train) #reading train.csv to pandas data frame

#Sum of null values
dataset_train.isnull().sum()

#Overiew of Data
dataset_train.head()

#List of columns present
list(dataset_train.columns)

#Number of Columns present
len(dataset_train.columns)

"""# **Data Visualizations**"""

#Histogram plot for the train data
dataset_train.hist(bins=50, figsize=(8, 6))
plt.suptitle("Data Distribution", fontsize=16)
plt.show()

"""Distance: The histogram for the "distance" feature appears to have a relatively normal distribution. The data is spread out around the central value, and the majority of the distances fall within a certain range. This indicates that the distances covered by the deliveries are evenly distributed, which is a positive characteristic for a logistic company's operations.

Cost: The histogram for the "cost" feature is right-skewed. This means that the majority of the delivery costs are clustered on the lower end, but there are a few deliveries with significantly higher costs that are pulling the distribution to the right. Right-skewed distributions are often associated with a long tail on the right side, indicating that some deliveries might be more expensive than others.

Weight: Similarly, the histogram for the "weight" feature is also right-skewed. This suggests that most deliveries have relatively low weights, while a smaller number of deliveries have higher weights. This could potentially indicate that there are a few heavy deliveries that contribute to the right skewness.
"""

# Figuring out outliers. Adding boxplot
dataset_train.boxplot(figsize=(20,10))
plt.xlabel("Columns")
plt.ylabel("Cost")
plt.title('Box Plot for finding Outliers')
plt.grid(True)
plt.show()

"""Distance: The "distance" feature shows a box with a median line inside. Since there are no outliers for the "distance" feature, the box is not extended by whiskers, and all data points within the range are contained within the box. This suggests that the distances covered by most deliveries are consistent and do not deviate significantly from the median distance.

Cost: The "cost" feature includes a box with a median line inside. The whiskers extend to data points that are considered potential outliers. Outliers in the "cost" feature indicate that some deliveries have unusually high costs compared to the majority of deliveries. These high-cost outliers might be influenced by specific circumstances, delivery requirements, or other factors.

Weight: Similarly, the "weight" feature exhibits a box plot with whiskers extending to potential outliers. Outliers in the "weight" feature suggest that there are deliveries with significantly higher weights than the majority of deliveries. These heavy-weight outliers may have specific implications for logistics operations.

**Plotting scatter_matrix**

Scatter matrix is a grid of scatter plots where each plot represents the relationship between two variables in a dataset. It helps visualize correlations and patterns between variables, aiding in exploratory data analysis and providing insights for modeling
"""

scatter_matrix(dataset_train)

#Plotting the Scatter plot for Distance and Cost
plt.scatter(dataset_train["distance"], dataset_train["cost"])
plt.xlabel("Distance")
plt.ylabel("Cost")
plt.title('Distance Vs Cost')
plt.figsize=(20,20)
plt.grid(True)
plt.show()

"""**Plotting line plot**

 Line plot is a simple and effective way to visualize the relationship between two variables by connecting data points with straight lines. It helps in identifying trends and patterns in the data
"""

sns.lineplot(x="distance", y="cost", data=dataset_train)
plt.xlabel("Distance")
plt.ylabel("Cost")
plt.title('Distance Vs Cost')
plt.grid(True)
plt.show()

"""#**Feature Engineering**"""

#Function to modify date
def formatDate(df):
  df['date'] = pd.to_datetime(df['date'])  # Convert 'dates' to datetime format
  df['day_of_week'] = df['date'].dt.dayofweek
  df['month'] = df['date'].dt.month
  df['year'] = df['date'].dt.year
  return df

formatDate(dataset_train)

#Function to find mode for categorical values and replace null values with the mode
def findModeAndApplyModeToNullValues(dataset):
  categoricalColumnsWithNullValues = ['exWeatherTag', 'type', 'packageType']
  for column in categoricalColumnsWithNullValues:
    if column == 'exWeatherTag':
      # Assuming the null values represents the normal weather conditions, it is being filled with the value 'normal'
      # Filling the null values with mode or any other imputation method could effect the model and prediction.
      mode_of_the_column = 'normal'
    else:
      mode_of_the_column = dataset[column].mode()
    dataset[column] = dataset[column].replace([np.NaN], mode_of_the_column)
  return dataset


def calculateMetrics(y_val, y_pred):
  # Calculate Mean Absolute Error (MAE)
  mae = mean_absolute_error(y_val, y_pred)
  print("Mean Absolute Error (MAE):", mae)

  # Calculate Mean Squared Error (MSE)
  mse = mean_squared_error(y_val, y_pred)
  print("Mean Squared Error (MSE):", mse)

  # Calculate Root Mean Squared Error (RMSE)
  rmse = mean_squared_error(y_val, y_pred, squared=False)
  print("Root Mean Squared Error (RMSE):", rmse)

  # Calculate R-squared (Coefficient of Determination)
  r2 = r2_score(y_val, y_pred)
  print("R-squared (Coefficient of Determination):", r2)

def adjusted_r_squared(y_val, y_pred, n, p):
  r2 = r2_score(y_val, y_pred)
  adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1))
  print("Adjusted R-squared:", adjusted_r2)
  return adjusted_r2

dataset_train = findModeAndApplyModeToNullValues(dataset_train)

#Ensuring there are no null values for the model training.
dataset_train.isnull().sum()

#Choosing a nre dataset_train which is described as dataset_train Modified as dataset_train_modified to drop the redundant values of Date, Trip ID
#Cost has been dropped in order to evaluate the training model in this train itself
dataset_train_modified = dataset_train
dataset_train_modified = dataset_train_modified.drop(['trip', 'date'],axis=1)

dataset_train_modified.describe()

dataset_train_modified =pd.get_dummies(dataset_train_modified, columns=['dayPart','carrier','exWeatherTag','originLocation','destinationLocation','type','packageType'])

dataset_train_modified

"""Keer code

# 3. Feature Scaling and test train split
"""

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder ,StandardScaler
from sklearn.model_selection import train_test_split

# Splitting the Data into Training and Testing Sets
train_set, test_set = train_test_split(dataset_train_modified, test_size=0.3, random_state=42)

# Defining target column as cost
target = 'cost'
features = list(train_set.columns)
features = [f for f in features if f!=target]

#features = [f for f in features if f!=target]
X_train = train_set[features]
y_train = train_set[[target]]

X_test = test_set[features]
y_test = test_set[[target]]

TARGET = "cost"
columns_predictors = [col for col in train_set.columns if col not in [TARGET]]
columns_categorical = ['dayPart_day',
 'dayPart_night',
 'carrier_A',
 'carrier_B',
 'carrier_C',
 'carrier_D',
 'exWeatherTag_heat',
 'exWeatherTag_normal',
 'exWeatherTag_snow',
 'originLocation_S1',
 'originLocation_S2',
 'originLocation_S3',
 'originLocation_S4',
 'originLocation_S5',
 'originLocation_S6',
 'originLocation_S7',
 'originLocation_S8',
 'originLocation_S9',
 'destinationLocation_D1',
 'destinationLocation_D2',
 'destinationLocation_D3',
 'destinationLocation_D4',
 'destinationLocation_D5',
 'destinationLocation_D6',
 'destinationLocation_D7',
 'destinationLocation_D8',
 'destinationLocation_D9',
 'type_expedited',
 'packageType_TT']
columns_numerical = [col for col in columns_predictors if col not in columns_categorical]
print(f"TARGET: {TARGET}")
print(f"columns_predictors: {columns_predictors}")
print(f"columns_categorical: {columns_categorical}")
print(f"columns_numerical: {columns_numerical}")

# Splitting the Data into Training and Testing Sets
columns_categorical_list = list(columns_categorical)
columns_numerical_list = list(columns_numerical)
pipeline_numerical = Pipeline([
  ('scaler', StandardScaler()),
])

pipeline_full = ColumnTransformer([
  ("numerical", pipeline_numerical, columns_numerical_list),
])

pipeline_full.fit(X_train)
X_train = pipeline_full.transform(X_train)
X_test = pipeline_full.transform(X_test)
print(f"X_train transformed.shape: {X_train.shape}")
print(f"X_test transformed.shape: {X_test.shape}")

"""#**Building Artifical Neural Network**

#Model 1
"""

#Keras Regressor

#Creating a new sequential neural network model using Keras.
Create_model = Sequential()

#Input Layer
Create_model.add(Dense(X_train.shape[1], activation='relu', input_dim = X_train.shape[1]))
#This adds an input layer to the model using the Dense layer from Keras. The X_train.shape[1]
#represents the number of features in the input data. The activation function used is ReLU ('relu'). input_dim specifies the number of input dimensions.

#Hidden Layer
#adding multiple hidden layers to the model with specified number of neurons and using the ReLU activation function.
#Initial weights of the neurons will be sampled from a normal distribution.
Create_model.add(Dense(512,kernel_initializer='normal', activation='relu'))
Create_model.add(Dense(512,kernel_initializer='normal', activation='relu'))
Create_model.add(Dense(256,kernel_initializer='normal', activation='relu'))
Create_model.add(Dense(128,kernel_initializer='normal', activation='relu'))
Create_model.add(Dense(64,kernel_initializer='normal', activation='relu'))
Create_model.add(Dense(32,kernel_initializer='normal', activation='relu'))

#Output Layer
Create_model.add(Dense(1,kernel_initializer='normal', activation = 'relu'))
#This adds the output layer to the model. Since this is a
#regression problem (predicting a continuous value), a single neuron in the output layer is used with the ReLU activation function.

Create_model.compile(loss = 'MeanSquaredError', optimizer='adam', metrics=['mse','mae'])
#This line compiles the model. It specifies the loss function ('MeanSquaredError') to be used during training,
# the optimizer ('adam') to update the weights based on the gradients, and additional metrics to monitor during training (['mse','mae']).
Create_model.summary()

tf.keras.utils.plot_model(Create_model, show_shapes=True)
#This line plots the model architecture, showing the shapes of input and output at each layer.

#library to use KerasClassifier
!pip install scikeras

from sklearn.model_selection import GridSearchCV
from scikeras.wrappers import KerasRegressor

batch_size = [50,75]
epochs = [3,5]

Hyp_Model_1 = KerasRegressor(model=Create_model)

param_grid = dict(batch_size=batch_size, epochs = epochs)
randSearch_1 = GridSearchCV(Hyp_Model_1, param_grid, cv=5, scoring='neg_mean_squared_error', error_score="raise", n_jobs=-1, verbose=0)

"""These lines import necessary libraries and set up hyperparameter tuning using GridSearchCV. It creates an instance of KerasRegressor with the Create_model defined earlier. Then it specifies a grid of hyperparameters to search through (batch_size and epochs). The GridSearchCV object randSearch_1 will perform a grid search over these hyperparameters to find the best combination that minimizes the negative mean squared error.

**Feature Importance based on weights**
"""

weights = Hyp_Model_1.model.layers[0].get_weights()[0]

# Calculate feature importance based on weights
feature_importance = np.sum(np.abs(weights), axis=1)

# Normalize the feature importance values
feature_importance /= np.max(feature_importance)

# Sort the features by importance
sorted_indices = np.argsort(feature_importance)

# Plot the feature importance
plt.barh(range(5), feature_importance[sorted_indices])
plt.yticks(range(5), sorted_indices)
plt.xlabel('Normalized Feature Importance')
plt.ylabel('Feature Index')
plt.title('Feature Importance based on Weights')
plt.show()

"""These lines calculate the feature importance based on the weights of the input layer neurons in the trained model. The absolute values of the weights are summed along each row (neuron), normalized, and then sorted to show the importance of each feature. A horizontal bar plot is used to visualize the feature importance. It represents the feature 2 has more impact and 3 has the less impact."""

import os
#Setting the minimum tensorflow loggin level to suppressing level to avoid displaying log messages except FATAL messages
os.environ["TF_CPP_MIN_LOG_LEVEL"]="5"
history = randSearch_1.fit(X_train, y_train,verbose=0)

# Getting the best combination of hyperparameters found during the search
best_params = randSearch_1.best_params_

# Getting the model with those hyperparameters that performed the best during the grid search.
best_estimators = randSearch_1.best_estimator_

print("Best params : ",best_params)
print("Best Estimators : ",best_estimators)

#This code uses the best estimator to make predictions on the test data (X_test) and assigns the predictions to y_prediction.
#Then, the calculateMetrics function is called to compute various metrics (mean absolute error, mean squared error, root mean squared error, and R-squared)
#by comparing the predicted values to the true values (y_test).
y_prediction = best_estimators.predict(X_test)
y_prediction

plt.scatter(y_prediction, y_test, color='blue', label='Predicted', marker='o')
plt.scatter(y_prediction, y_prediction, color='red', label='Actual', marker='x')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Model 1 Predictions vs Actual Values')
plt.legend()
plt.show()

calculateMetrics(y_prediction, y_test)
adjusted_r_squared(y_prediction, y_test,n=(len(y_test)+len(y_train)),p=len(y_prediction))
#Calculating the Error values

"""#Model 2

**Model 2:**
Applying L1 and L2 regularization. Regularization helps prevent overfitting by adding a penalty term to the loss function based on the magnitude of the weights. Adding dropout layer to prevent overfitting during training.
"""

Create_model_2 = Sequential()

#Input Layer
Create_model_2.add(Dense(X_train.shape[1], activation='relu', input_dim = X_train.shape[1]))

#Hidden Layer
#Applying L1 regularization on the kernel weights and L2 regularization on the bias weights.
Create_model_2.add(Dense(512,kernel_initializer='normal', activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01), bias_regularizer=tf.keras.regularizers.l2(0.015)))

#Adding dropout layer to randomly drops a fraction of neurons during training to prevent overfitting.
Create_model_2.add(Dropout(0.3))

Create_model_2.add(Dense(256,kernel_initializer='normal', activation='relu'))
Create_model_2.add(Dropout(0.3))
Create_model_2.add(Dense(128,kernel_initializer='normal', activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01), bias_regularizer=tf.keras.regularizers.l2(0.015)))
Create_model_2.add(Dropout(0.3))
Create_model_2.add(Dense(64,kernel_initializer='normal', activation='relu'))
Create_model_2.add(Dropout(0.3))
Create_model_2.add(Dense(32,kernel_initializer='normal', activation='relu'))
Create_model_2.add(Dropout(0.3))
#Output Layer
Create_model_2.add(Dense(1,kernel_initializer='normal', activation = 'relu'))

#Creating an instance of the Adam optimizer with a learning rate of 0.01.
opt = keras.optimizers.Adam(learning_rate=0.01)

Create_model_2.compile(loss = 'MeanSquaredError', optimizer=opt, metrics=['mse','mae'])

#Printing thr summary of the model architecture, showing the number of parameters and the structure of each layer.
Create_model_2.summary()

"""These lines create a new sequential neural network model Create_model_2. An input layer is added to the model, similar to the previous model. The activation function used is ReLU ('relu'), and input_dim is set to the number of features in the training data. Hidden layer to the model with 512 neurons and ReLU activation function. It applies L1 regularization to the kernel weights (kernel_regularizer) with a regularization strength of 0.01, and L2 regularization to the bias weights (bias_regularizer) with a regularization strength of 0.015. A dropout layer is added after the first hidden layer. Dropout randomly drops a fraction of neurons during each training iteration, which helps prevent overfitting. More hidden layers to the model, each followed by a dropout layer. The structure is similar to the previous hidden layer. The dropout rate is set to 0.3, which means that during training, approximately 30% of the neurons in the dropout layers will be randomly set to zero.

Early stopping
"""

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='mse', verbose=1, patience=5)

history = Create_model_2.fit(X_train, y_train, batch_size=25, epochs=100, callbacks=[es])

"""An EarlyStopping callback is created, which monitors the mean squared error ('mse') on the validation set. If the validation mse does not improve for 5 consecutive epochs, training will stop early. The model is trained using the fit method, where X_train and y_train are the training data and labels, respectively. A batch size of 25 and a maximum of 100 epochs are specified. The callbacks parameter includes the EarlyStopping callback.

**Feature Importance for model 2 based on weights**
"""

weights = Create_model_2.layers[0].get_weights()[0]

# Calculate feature importance based on weights
#feature_importance = np.sum(np.abs(weights), axis=1)
feature_importance = np.abs(weights).sum(axis=1)
print(feature_importance)
# Normalize the feature importance values
#feature_importance /= np.max(feature_importance)


# Sort the features by importance
sorted_indices = np.argsort(feature_importance)

# Plot the feature importance
plt.barh(range(5), feature_importance[sorted_indices])
plt.yticks(range(5), sorted_indices)
plt.xlabel('Normalized Feature Importance')
plt.ylabel('Feature Index')
plt.title('Feature Importance based on Weights')
plt.show()

"""These lines calculate the feature importance based on the weights of the input layer neurons in the trained model. The absolute values of the weights are summed along each row (neuron), normalized, and then sorted to show the importance of each feature. A horizontal bar plot is used to visualize the feature importance. It represents the feature 1 has more impact and 2 has the less impact."""

from matplotlib import pyplot

train_acc = Create_model_2.evaluate(X_train, y_train)
test_acc = Create_model_2.evaluate(X_test, y_test)
model_2_prediction = Create_model_2.predict(X_test)

calculateMetrics(model_2_prediction, y_test)
adjusted_r_squared(model_2_prediction, y_test,n=(len(y_test)+len(y_train)),p=len(model_2_prediction))
model_2_prediction

# Plot actual vs. predicted values

plt.scatter(model_2_prediction, y_test, color='blue', label='Predicted', marker='o')
plt.scatter(model_2_prediction,model_2_prediction, color='red', label='Actual', marker='x')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Model 1 Predictions vs Actual Values')
plt.legend()
plt.show()

"""#Model 3

**Model 3:**
Model3 has 3 hidden layers with the elu activation function, and we use the RMSprop optimizer with learning rate=0.0005.
"""

# Third model - 3 hidden layers with elu activation, rmsprop optimizer with learning rate=0.0005
model3 = tf.keras.models.Sequential()
model3.add(Dense(X_train.shape[1], activation='relu', input_dim = X_train.shape[1]))
model3.add(tf.keras.layers.Dense(256, activation='elu'))
model3.add(tf.keras.layers.Dense(64, activation='elu'))
model3.add(tf.keras.layers.Dense(1, activation='sigmoid'))
opt3 = tf.keras.optimizers.RMSprop(learning_rate=0.0005)

model3.compile(optimizer=opt3, loss='binary_crossentropy', metrics=['mse','mae'])
model3.summary()

"""These lines create the third model, referred to as model3. It's a sequential neural network model. Here's a breakdown of the layers added to the model:

An input layer is added, similar to the previous models. It has the same number of neurons as the features in the training data (X_train.shape[1]), and the activation function used is ReLU ('relu').

A hidden layer is added with 256 neurons and the Exponential Linear Unit (ELU) activation function ('elu'). ELU is another type of activation function that can help mitigate the vanishing gradient problem.

Another hidden layer is added with 64 neurons and the ELU activation function.

The output layer is added with a single neuron and the sigmoid activation function. The sigmoid activation is used in binary classification problems to produce a probability output.

The RMSprop optimizer is used with a learning rate of 0.0005. The loss function is set to 'binary_crossentropy', which is commonly used for binary classification problems. The metrics 'mse' (mean squared error) and 'mae' (mean absolute error) are specified for monitoring during training. The summary() function is used to print a summary of the model's architecture.
"""

es2 = EarlyStopping(monitor='mse', verbose=1, patience=5)

history_3 = model3.fit(X_train, y_train, batch_size=25, epochs=50, callbacks=[es2])

#Model 3 Prediction
model_3_prediction = model3.predict(X_test)
model_3_prediction[0:20]

"""#Model 4

Reduced the Hidden Layers.
"""

Create_model_4 = Sequential()

#Input Layer
Create_model_4.add(Dense(X_train.shape[1], activation='relu', input_dim = X_train.shape[1]))

#Hidden Layer
#Applying L1 regularization on the kernel weights and L2 regularization on the bias weights.
Create_model_4.add(Dense(512,kernel_initializer='normal', activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01), bias_regularizer=tf.keras.regularizers.l2(0.015)))

#Adding dropout layer to randomly drops a fraction of neurons during training to prevent overfitting.
Create_model_4.add(Dropout(0.3))

Create_model_4.add(Dense(256,kernel_initializer='normal', activation='relu'))
Create_model_4.add(Dropout(0.3))
Create_model_4.add(Dense(128,kernel_initializer='normal', activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01), bias_regularizer=tf.keras.regularizers.l2(0.015)))
Create_model_4.add(Dropout(0.3))



#Output Layer
Create_model_4.add(Dense(1,kernel_initializer='normal', activation = 'relu'))

#Creating an instance of the Adam optimizer with a learning rate of 0.01.
opt = keras.optimizers.Adam(learning_rate=0.01)

Create_model_4.compile(loss = 'MeanSquaredError', optimizer=opt, metrics=['mse','mae'])

#Printing thr summary of the model architecture, showing the number of parameters and the structure of each layer.
Create_model_4.summary()

#Early stopping
#------------------------
from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='mse', verbose=1, patience=5)

history = Create_model_4.fit(X_train, y_train, batch_size=25, epochs=100, callbacks=[es])

weights = Create_model_4.layers[0].get_weights()[0]

# Calculate feature importance based on weights
#feature_importance = np.sum(np.abs(weights), axis=1)
feature_importance = np.abs(weights).sum(axis=1)
print(feature_importance)
# Normalize the feature importance values
#feature_importance /= np.max(feature_importance)


# Sort the features by importance
sorted_indices = np.argsort(feature_importance)

# Plot the feature importance
plt.barh(range(5), feature_importance[sorted_indices])
plt.yticks(range(5), sorted_indices)
plt.xlabel('Normalized Feature Importance')
plt.ylabel('Feature Index')
plt.title('Feature Importance based on Weights')
plt.show()

#result

from matplotlib import pyplot

train_acc = Create_model_4.evaluate(X_train, y_train)
test_acc = Create_model_4.evaluate(X_test, y_test)
model_4_prediction = Create_model_4.predict(X_test)

calculateMetrics(model_4_prediction, y_test)
adjusted_r_squared(model_4_prediction, y_test,n=(len(y_test)+len(y_train)),p=len(model_4_prediction))

model_4_prediction

# Plot actual vs. predicted values
plt.scatter(model_4_prediction, y_test, color='blue', label='Predicted', marker='o')
plt.scatter(model_4_prediction, model_4_prediction, color='red', label='Actual', marker='x')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Model 4 Predictions vs Actual Values')
plt.legend()
plt.show()

from tabulate import tabulate

# Define the data for the table
data = [
    ["Model 1", 21.301145201617476, 3759.1651448692173, 61.312030996120306, 0.8508246760497982, 0.7868877103300619],
    ["Model 2", 29.119678484694298, 7401.742377486728, 86.03337943778989, 0.5789954918059359, 0.39855176897384015],
    ["Model 3", '-', '-', '-', '-', '-'],
    ["Model 4", 18.971184768925813, 4990.086899140178, 70.64054713222554, 0.8754029693398925, 0.8220003296328349]
]

# Define the headers for the table
headers = ["Model", "Mean Absolute Error (MAE)", "Mean Squared Error (MSE)", "Root Mean Squared Error (RMSE)", "R-squared", "Adjusted R-squared"]

# Print the table
table = tabulate(data, headers=headers, tablefmt="grid")
print(table)

"""#Test Set

**Until now the code is done for the Train set. But the test set has to be prepared for it.**

**The following are for the Test set**
"""

dataframe_test = pd.read_csv("/content/sample_data/test.csv")
#dataframe_test = pd.read_csv(file_url_test)
dataframe_test

formatDate(dataframe_test)

"""# **Data Preprocessing for Test set**"""

#Dropping the columns
dfMT = dataframe_test.drop(['trip','date'],axis=1)
dfMT = findModeAndApplyModeToNullValues(dfMT)

#Doing one-hot encoding to fill/replace the values with feasible values to train the model
dfMT =pd.get_dummies(dfMT, columns=['dayPart','carrier','exWeatherTag','originLocation','destinationLocation','type','packageType'])

#MinMaxScaler expects a 2D array as input for the fit_transform() method. However, the 'distance' column you provided is in 1D form
# Reshape the 'distance' column to a 2D array
distance_2d = dfMT['distance'].values.reshape(-1, 1)

dfMT

#Getting no of columns in the Train data frame Modified

columns_list = train_set.columns.tolist()

print(columns_list)

#Getting no of columns in the Test data frame Modified

columns_list = dfMT.columns.tolist()

print(columns_list)

common_columns = train_set.columns.intersection(dfMT.columns)
print(common_columns)

dfMT

# Splitting the Data into Training and Testing Sets
columns_categorical_list = list(columns_categorical)
columns_numerical_list = list(columns_numerical)
pipeline_numerical = Pipeline([
  ('scaler', StandardScaler()),
])

pipeline_full_test = ColumnTransformer([
  ("numerical", pipeline_numerical, columns_numerical_list),
])

# Fit the feature scaling pipeline on the test data
pipeline_full_test.fit(dfMT)

# Transform the test data using the fitted pipeline
transform_dfMT = pipeline_full.transform(dfMT)

# Print the shape of the transformed test data
print(f"dfMT transformed.shape: {transform_dfMT.shape}")

# Predictions using different models on the transformed test data
model_1_prediction = best_estimators.predict(transform_dfMT)
model_1_prediction[0:20]

model_2_prediction = Create_model_2.predict(transform_dfMT)
model_2_prediction[0:20]

